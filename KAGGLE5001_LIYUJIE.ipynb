{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Ana\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (20,32,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "D:\\Anaconda\\Ana\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (18,21,25,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThis data comes from https://www.ncei.noaa.gov/access/search/data-search/global-hourly?bbox=22.363,113.843,22.212,114.067&startDate=2017-01-01T00:00:00&endDate=2018-12-31T23:59:59\\nSome weather information about Hong Kong Airport are inclued in this two files.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "tmp_data_2017=pd.read_csv(\"E:\\\\5001\\\\KAGGLE\\\\2017_data.csv\")\n",
    "tmp_data_2018=pd.read_csv(\"E:\\\\5001\\\\KAGGLE\\\\2018_data.csv\")\n",
    "'''\n",
    "This data comes from https://www.ncei.noaa.gov/access/search/data-search/global-hourly?bbox=22.363,113.843,22.212,114.067&startDate=2017-01-01T00:00:00&endDate=2018-12-31T23:59:59\n",
    "Some weather information about Hong Kong Airport are inclued in this two files.\n",
    "'''\n",
    "#print(tmp_data_2017.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGet corresponding Datetime information from each row of this two files.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dt_transformation(dt):\n",
    "    dt[\"datetime_1\"]=dt[\"DATE\"].apply(lambda x:datetime.datetime.strptime(x[:13],\"%Y-%m-%dT%H\"))\n",
    "    return dt\n",
    "tmp_data_2017=dt_transformation(tmp_data_2017)\n",
    "tmp_data_2018=dt_transformation(tmp_data_2018)\n",
    "'''\n",
    "Get corresponding Datetime information from each row of this two files.\n",
    "'''\n",
    "#print(tmp_data_2017.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytz import utc  \n",
    "from pytz import timezone \n",
    "\n",
    "def timezone_transfermation(dt):\n",
    "    cst_tz = timezone('Asia/Shanghai')  \n",
    "    utc_tz = timezone('UTC') \n",
    "    dt[\"datetime\"]=dt[\"datetime_1\"].apply(lambda x:x.replace(tzinfo=utc_tz).astimezone(cst_tz))\n",
    "    return dt\n",
    "    \n",
    "'''\n",
    "Due to original TimeZone is UTC, we need to set the TimeZone as UTC+8 here.\n",
    "'''    \n",
    "\n",
    "def temperature_transfermation(dt):\n",
    "    dt[\"Temp\"]=dt[\"TMP\"].apply(lambda x:float(x[2:5]))\n",
    "    return dt\n",
    "    \n",
    "'''\n",
    "Get Temperature of Hong Kong Airport.\n",
    "'''\n",
    "    \n",
    "tmp_data_2017=timezone_transfermation(tmp_data_2017)\n",
    "tmp_data_2018=timezone_transfermation(tmp_data_2018)\n",
    "tmp_data_2017=temperature_transfermation(tmp_data_2017)\n",
    "tmp_data_2018=temperature_transfermation(tmp_data_2018)\n",
    "tmp_data_2017=tmp_data_2017[[\"datetime\",\"Temp\"]]\n",
    "tmp_data_2018=tmp_data_2018[[\"datetime\",\"Temp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"E:\\\\5001\\\\KAGGLE\\\\train.csv\")\n",
    "test_data=pd.read_csv(\"E:\\\\5001\\\\KAGGLE\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_data.copy()\n",
    "test_dataset=test_data.copy()\n",
    "    \n",
    "def datetime_transfermation(dataset):\n",
    "    dataset['datetime']=dataset['date'].apply(lambda x:datetime.datetime.strptime(x,\"%d/%m/%Y %H:00\"))\n",
    "    return dataset\n",
    "    \n",
    "def get_day_month_year(dataset):\n",
    "    dataset['month']=dataset['datetime'].apply(lambda x:int(x.month))\n",
    "    dataset['day']=dataset['datetime'].apply(lambda x:int(x.day))\n",
    "    dataset['year']=dataset['datetime'].apply(lambda x:int(x.year))\n",
    "    return dataset\n",
    "    \n",
    "def get_hour(dataset):\n",
    "    dataset['hour']=dataset['datetime'].apply(lambda x:int(x.hour))\n",
    "    return dataset\n",
    "\n",
    "def get_hour_feature(dataset,feature_select=True):\n",
    "    '''\n",
    "    Every day is divided into different periods.\n",
    "    '''\n",
    "    if feature_select:\n",
    "        def get_feature(x):\n",
    "            time_range=[[0,1,2,3],[4,5,6,7],[8,9],[10,11,12],[13,14,15,16],[17,18,19],[20,21,22,23]]\n",
    "            for i in range(len(time_range)):\n",
    "                if x in time_range[i]:\n",
    "                    return chr(i+97)\n",
    "        dataset['hour_t']=dataset['hour'].apply(get_feature)\n",
    "    else:\n",
    "        dataset['hour_t']=dataset['hour'].apply(lambda x: chr(x+97))\n",
    "    return dataset\n",
    "\n",
    "def get_Weekday(dataset):\n",
    "    dataset['Sun']=dataset['datetime'].apply(lambda x: 1 if x.weekday()==6 else 0) \n",
    "    dataset['Sat']=dataset['datetime'].apply(lambda x: 1 if x.weekday()==5 else 0)\n",
    "    dataset['Fri']=dataset['datetime'].apply(lambda x: 1 if x.weekday()==4 else 0)\n",
    "    dataset['Thu']=dataset['datetime'].apply(lambda x: 1 if x.weekday()==3 else 0) \n",
    "    dataset['Wed']=dataset['datetime'].apply(lambda x: 1 if x.weekday()==2 else 0)\n",
    "    dataset['Tue']=dataset['datetime'].apply(lambda x: 1 if x.weekday()==1 else 0)\n",
    "    dataset['Mon']=dataset['datetime'].apply(lambda x: 1 if x.weekday()==0 else 0)\n",
    "    return dataset\n",
    "\n",
    "def get_weekday_second_edition(dataset):\n",
    "    dataset[\"weekday\"]=dataset[\"datetime\"].apply(lambda x: x.weekday())\n",
    "    return dataset\n",
    "\n",
    "def get_public_holiday(dataset):\n",
    "    holiday_list= ['2017-01-02','2017-01-28','2017-01-30','2017-01-31','2017-04-04','2017-04-14',\n",
    "           '2017-04-15','2017-04-17','2017-05-01','2017-05-03','2017-05-30','2017-07-01',\n",
    "           '2017-10-02','2017-10-05','2017-10-28','2017-12-25','2017-12-26','2018-01-01',\n",
    "           '2018-02-16','2018-02-17','2018-02-19','2018-03-30','2018-03-31','2018-04-02',\n",
    "           '2018-04-05','2018-05-01','2018-05-22','2018-06-18','2018-07-02','2018-09-25',\n",
    "           '2018-10-01','2018-10-17','2018-12-25','2018-12-26']\n",
    "    dataset['holiday']=dataset['datetime'].apply(lambda x:1 if x.strftime('%Y-%m-%d') in holiday_list else 0)\n",
    "    return dataset\n",
    "    \n",
    "def dataset_transformation_operations(train_dataset): \n",
    "    train_dataset=datetime_transfermation(train_dataset)\n",
    "    train_dataset=get_day_month_year(train_dataset)\n",
    "    train_dataset=get_hour(train_dataset)\n",
    "    train_dataset=get_Weekday(train_dataset)\n",
    "    train_dataset=get_weekday_second_edition(train_dataset)\n",
    "    train_dataset=get_hour_feature(train_dataset,True)\n",
    "    train_dataset=get_public_holiday(train_dataset)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Do Same Operation to both train_data and test_data\n",
    "'''\n",
    "train_dataset=dataset_transformation_operations(train_dataset)\n",
    "test_dataset=dataset_transformation_operations(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id           date      speed            datetime  month  day  year  hour  \\\n",
      "0   0  1/1/2017 0:00  43.002930 2017-01-01 00:00:00      1    1  2017     0   \n",
      "1   1  1/1/2017 1:00  46.118696 2017-01-01 01:00:00      1    1  2017     1   \n",
      "2   2  1/1/2017 2:00  44.294158 2017-01-01 02:00:00      1    1  2017     2   \n",
      "3   3  1/1/2017 3:00  41.067468 2017-01-01 03:00:00      1    1  2017     3   \n",
      "4   4  1/1/2017 4:00  46.448653 2017-01-01 04:00:00      1    1  2017     4   \n",
      "\n",
      "   Sun  Sat  Fri  Thu  Wed  Tue  Mon  weekday hour_t  holiday  \n",
      "0    1    0    0    0    0    0    0        6      a        0  \n",
      "1    1    0    0    0    0    0    0        6      a        0  \n",
      "2    1    0    0    0    0    0    0        6      a        0  \n",
      "3    1    0    0    0    0    0    0        6      a        0  \n",
      "4    1    0    0    0    0    0    0        6      b        0  \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id           date      speed                  datetime  month  day  year  \\\n",
      "0   0  1/1/2017 0:00  43.002930 2017-01-01 00:00:00+08:00      1    1  2017   \n",
      "1   1  1/1/2017 1:00  46.118696 2017-01-01 01:00:00+08:00      1    1  2017   \n",
      "2   2  1/1/2017 2:00  44.294158 2017-01-01 02:00:00+08:00      1    1  2017   \n",
      "3   3  1/1/2017 3:00  41.067468 2017-01-01 03:00:00+08:00      1    1  2017   \n",
      "4   4  1/1/2017 4:00  46.448653 2017-01-01 04:00:00+08:00      1    1  2017   \n",
      "\n",
      "   hour  Sun  Sat  Fri  Thu  Wed  Tue  Mon  weekday hour_t  holiday  \n",
      "0     0    1    0    0    0    0    0    0        6      a        0  \n",
      "1     1    1    0    0    0    0    0    0        6      a        0  \n",
      "2     2    1    0    0    0    0    0    0        6      a        0  \n",
      "3     3    1    0    0    0    0    0    0        6      a        0  \n",
      "4     4    1    0    0    0    0    0    0        6      b        0  \n"
     ]
    }
   ],
   "source": [
    "def set_timezone(dt):\n",
    "    cst_tz = timezone('Asia/Shanghai')  \n",
    "    utc_tz = timezone('Asia/Shanghai') \n",
    "    dt[\"datetime\"]=dt[\"datetime\"].apply(lambda x:x.replace(tzinfo=utc_tz).astimezone(cst_tz))\n",
    "    return dt\n",
    "\n",
    "def get_merge_result(dt,data_2017,data_2018):\n",
    "    data_2017.drop_duplicates(subset=['datetime'],keep='first',inplace=True)\n",
    "    data_2018.drop_duplicates(subset=['datetime'],keep='first',inplace=True)\n",
    "    result=pd.merge(dt,data_2017,how='left',on=['datetime'])\n",
    "    result=pd.merge(result,data_2018,how='left',on=['datetime'])\n",
    "    return result\n",
    "'''\n",
    "Merge Tables include temperature information and table include other information(speed, datetime, and so on).\n",
    "'''\n",
    "train_dataset=set_timezone(train_dataset)\n",
    "print(train_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14006\n",
      "   id           date      speed                  datetime  month  day  year  \\\n",
      "0   0  1/1/2017 0:00  43.002930 2017-01-01 00:00:00+08:00      1    1  2017   \n",
      "1   1  1/1/2017 1:00  46.118696 2017-01-01 01:00:00+08:00      1    1  2017   \n",
      "2   2  1/1/2017 2:00  44.294158 2017-01-01 02:00:00+08:00      1    1  2017   \n",
      "3   3  1/1/2017 3:00  41.067468 2017-01-01 03:00:00+08:00      1    1  2017   \n",
      "4   4  1/1/2017 4:00  46.448653 2017-01-01 04:00:00+08:00      1    1  2017   \n",
      "\n",
      "   hour  Sun  Sat  Fri  Thu  Wed  Tue  Mon  weekday hour_t  holiday  Temp_x  \\\n",
      "0     0    1    0    0    0    0    0    0        6      a        0     NaN   \n",
      "1     1    1    0    0    0    0    0    0        6      a        0     NaN   \n",
      "2     2    1    0    0    0    0    0    0        6      a        0     NaN   \n",
      "3     3    1    0    0    0    0    0    0        6      a        0     NaN   \n",
      "4     4    1    0    0    0    0    0    0        6      b        0     NaN   \n",
      "\n",
      "   Temp_y  \n",
      "0     NaN  \n",
      "1     NaN  \n",
      "2     NaN  \n",
      "3     NaN  \n",
      "4     NaN  \n"
     ]
    }
   ],
   "source": [
    "result=get_merge_result(train_dataset,tmp_data_2017,tmp_data_2018)\n",
    "print(len(result))\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train_dataset))\n",
    "#print(result[\"Temp_x\"])\n",
    "#print(result[\"Temp_y\"])\n",
    "a=list(result[\"Temp_x\"])\n",
    "b=list(result[\"Temp_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id           date      speed                  datetime  month  day  year  \\\n",
      "0   0  1/1/2017 0:00  43.002930 2017-01-01 00:00:00+08:00      1    1  2017   \n",
      "1   1  1/1/2017 1:00  46.118696 2017-01-01 01:00:00+08:00      1    1  2017   \n",
      "2   2  1/1/2017 2:00  44.294158 2017-01-01 02:00:00+08:00      1    1  2017   \n",
      "3   3  1/1/2017 3:00  41.067468 2017-01-01 03:00:00+08:00      1    1  2017   \n",
      "4   4  1/1/2017 4:00  46.448653 2017-01-01 04:00:00+08:00      1    1  2017   \n",
      "\n",
      "   hour  Sun  Sat  ...  Thu  Wed  Tue  Mon  weekday  hour_t holiday  Temp_x  \\\n",
      "0     0    1    0  ...    0    0    0    0        6       a       0     NaN   \n",
      "1     1    1    0  ...    0    0    0    0        6       a       0     NaN   \n",
      "2     2    1    0  ...    0    0    0    0        6       a       0     NaN   \n",
      "3     3    1    0  ...    0    0    0    0        6       a       0     NaN   \n",
      "4     4    1    0  ...    0    0    0    0        6       b       0     NaN   \n",
      "\n",
      "   Temp_y   Temp  \n",
      "0     NaN  190.0  \n",
      "1     NaN  190.0  \n",
      "2     NaN  190.0  \n",
      "3     NaN  190.0  \n",
      "4     NaN  190.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGet Temperature information for each row in train table. \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "c=[]\n",
    "for i in range(len(a)):\n",
    "    if not math.isnan(a[i]):\n",
    "        c.append(a[i])\n",
    "    else:\n",
    "        c.append(b[i])\n",
    "for i in range(8):\n",
    "    c[i]=190.0\n",
    "result[\"Temp\"]=c\n",
    "print(result.head())\n",
    "'''\n",
    "Get Temperature information for each row in train table. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFeatures will be used in training stage.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_vars=['hour_t','month','day','holiday','Sun']\n",
    "number_vars=[\"Temp\",'year','Sat','Mon','Fri','weekday','hour']\n",
    "'''\n",
    "Features will be used in training stage.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer,OneHotEncoder,StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "n_transformation=Pipeline(steps=[('scaler', StandardScaler())])\n",
    "c_transformation=Pipeline(steps=[('oneHot',OneHotEncoder())])\n",
    "\n",
    "preprocessor=ColumnTransformer(transformers=[('num',n_transformation,number_vars),\n",
    "    ('cat',c_transformation,category_vars)])\n",
    "\n",
    "train_dataset_m=preprocessor.fit_transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n10% of data are used for testing.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(train_dataset_m,train_dataset['speed'],test_size=0.1,random_state=980521)\n",
    "'''\n",
    "10% of data are used for testing.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12605, 61)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=2)]: Done  96 out of  96 | elapsed:  5.4min finished\n",
      "D:\\Anaconda\\Ana\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "param_grid = {'nthread':[4], \n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate':[.03,.05],\n",
    "              'max_depth':[4,5,6],\n",
    "              'min_child_weight':[2,3,4,5],\n",
    "              'subsample':[0.8],\n",
    "              'colsample_bytree':[0.8],\n",
    "              'gamma':[0],\n",
    "              'reg_lambda':[1],\n",
    "              'reg_alpha':[0],\n",
    "              'n_estimators':[1200]}\n",
    "'''\n",
    "After many attempts, we finally determined a smaller range for grid search to obtain the best parameters\n",
    "'''\n",
    "GS=GridSearchCV(estimator=model,param_grid=param_grid,cv=4,verbose=4,n_jobs=2)\n",
    "GS.fit(X_train,y_train)\n",
    "y_pred=GS.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.03, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 1200, 'nthread': 4, 'objective': 'reg:squarederror', 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "12.762005160984351\n"
     ]
    }
   ],
   "source": [
    "print(\"{0}\".format(GS.best_params_))\n",
    "print(MSE(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGet and Save the final result.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset=set_timezone(test_dataset)\n",
    "test_result=get_merge_result(test_dataset,tmp_data_2017,tmp_data_2018)\n",
    "a=list(test_result[\"Temp_x\"])\n",
    "b=list(test_result[\"Temp_y\"])\n",
    "import math\n",
    "c=[]\n",
    "for i in range(len(a)):\n",
    "    if not math.isnan(a[i]):\n",
    "        c.append(a[i])\n",
    "    else:\n",
    "        c.append(b[i])\n",
    "        \n",
    "test_result[\"Temp\"]=c\n",
    "\n",
    "'''\n",
    "Do same operation to test data.\n",
    "'''\n",
    "\n",
    "test_dataset_m=preprocessor.fit_transform(test_result)\n",
    "y_pred_test=GS.predict(test_dataset_m)\n",
    "y_p=list(y_pred_test)\n",
    "pred=pd.DataFrame({'speed':y_p},columns=['speed'])\n",
    "result=pd.concat([test_data.drop('date',axis=1),pred],axis=1)\n",
    "result.to_csv('E:\\\\5001\\\\KAGGLE\\\\Result_BOGUS_final.csv',index=False)\n",
    "'''\n",
    "Get and Save the final result.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
